abort & 4 & success & Test abort, rank 0 will abort right away, other processes will sleep and should not exit with code 0 \\ 
accept\_user\_arguments & 1 & success & Tests that we can accept user arguments \\ 
allgather & 10 & success & Test allgather, gathers rank from all processes and distributes to all \\ 
allreduce & 10 & success & Allreduce, actually computes factorial of mpi\_size \\ 
alltoall & 10 & success & Test alltoall, sending individual data from all to all \\ 
barrier & 11 & success & Test if barriers work and do so tightly \\ 
bcast & 11 & success & Test if broadcast works \\ 
big\_messages & 2 & success & Tests how we deal with transmitting very large messages \\ 
breaking & 2 & success & Tests if that crashes in MPI program are propagated back to initiator \\ 
cartesian\_topology & 8 & success & Baseline test of cartesian topologies (use CartesianTest class to be thorough) \\ 
collective\_ops\_invalid\_rank & 4 & success & Test all collective operations receiving a root parameter with invalid root \\ 
communicator-ops & 2 & success & Test if communicator operations function properly \\ 
finalize\_quickly & 8 & success & The minimal correct pupyMPI program does nothing but finalize \\ 
gather & 10 & success & Gather test, receive information from all processes to one \\ 
groups & 2 & success & Test simple groups functionality \\ 
groups2 & 10 & success & Test advanced groups functionality \\ 
initialized & 2 & success & Test establishment of MPI environment \\ 
isend\_to\_irecv & 2 & success & Test out of order immediate sending/receiving between 2 processses \\ 
isend\_to\_recv & 2 & success & Simple pupympi program to test basic immediate send to blocking recieve \\ 
issend\_slow\_recv & 2 & success & Test issend with delayed matching receive \\ 
lock\_trouble & 2 & success & Basic send and recive over many iterations where communicating parties should remain in sync \\ 
numpy\_bcast & 2 & success & Testing the broadcast function with different numpy data types \\ 
numpy\_reduce & 10 & success & A allreduce showing that numpy objects can be transferred with collective operations \\ 
numpy\_send\_types & 2 & success & A simple test and receive showing that numpy objects can be transferred without problems \\ 
rank\_and\_size & 2 & success & Simple pupympi hello world'ish program used for examples \\ 
reduce & 10 & success & Test reduce, first compute factorial of mpi\_size, then compare builtin ops max, min, sum to Python built-ins \\ 
reduce\_lists & 10 & success & Test allreduce with list as main type and a custom reduce op \\ 
reduce\_lists2 & 4 & success & Reduce with list as type \\ 
request\_cancelled & 2 & success & Test that irecv and isend request can be cancelled \\ 
scan & 10 & success & Test that scan makes a partial reduce \\ 
scatter & 10 & success & Scatter test with various data sizes \\ 
scatter\_invalid\_data & 4 & success & Scatter testing invalid data exceptions \\ 
scatter\_list & 4 & success & Test of list scattering \\ 
self\_communication & 2 & success & Test communication with self \\ 
send\_0\_fix\_none & 10 & success & Test that "", [] () and 0 all result in None when sent \\ 
send\_to\_irecv & 2 & success & Test send to irecv with out of order recieving \\ 
send\_to\_recv & 2 & success & Test simple send to recv between 2 processes \\ 
sendreceive & 5 & success & Testing the sendrecv call. All processes pass a token around \\ 
simple\_blocking\_sr & 2 & success & Test basic blocking communication. First rank 0 sends then 1 recieves, then vice versa. \\ 
simple\_immediate\_sr & 2 & success & Test basic immediate communication. First rank 0 isends timestamp to rank 1 who is a very slow reciever so rank 0 should quit early \\ 
simple\_synchronized\_sr & 2 & success & Test basic synchronized communication. First rank 0 sends then 1 recieves, then vice versa. \\ 
small\_collective & 10 & success & Test basic collective operation (gather) \\ 
ssend\_slow\_recv & 2 & success & Test ssend with delayed matching receive \\ 
stress\_sr & 2 & success & Cyclic blocking send/receive between two processes. Runs 500 iterations, and verifies that the data received are correct. (at current timeout-bound design, 500 iterations can take about 600 seconds) \\ 
stress\_sr2 & 5 & success & Multi-process version of stress\_sr, with 200 iterations. Processes communicate p2p with neighbours in lockstep: Evens send and odds recieve then vice versa. If odd number of processes are specified the last ranking one is  excluded to avoid deadlock \\ 
tags\_source & 2 & success & Test if tags/sources are handled correctly. Rank 0 sends 5 messages to rank 1 which should receive the first 3 but not the last 2. \\ 
testall & 2 & success & Test of testall \\ 
testany & 4 & success & Test of testany \\ 
testsome & 4 & success & Test of testsome \\ 
waitall & 2 & success & Test that waitall returns list of results \\ 
waitany & 2 & success & Test waitany on request list \\ 
waitfunctions & 10 & success & Tests waitall returning data \\ 
waitsome & 3 & success & Test waitsome on list of request handles \\ 
